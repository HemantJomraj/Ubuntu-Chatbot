{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"JFNHhsaWFb0C","colab_type":"code","outputId":"1b75a735-8571-4d65-b33d-2b5b3d23696f","executionInfo":{"status":"ok","timestamp":1554814841066,"user_tz":-330,"elapsed":40626,"user":{"displayName":"Darshan Kakwani","photoUrl":"","userId":"06813604131485796128"}},"colab":{"base_uri":"https://localhost:8080/","height":731}},"cell_type":"code","source":["!pip install tensorflow==1.8"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n","\u001b[K    100% |████████████████████████████████| 49.1MB 697kB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (0.33.1)\n","Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (3.7.1)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (0.7.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (1.14.6)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (0.7.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (1.15.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (1.11.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (1.1.0)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8) (0.2.2)\n","Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow==1.8)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n","\u001b[K    100% |████████████████████████████████| 3.1MB 10.0MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8) (40.9.0)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8) (0.15.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8) (3.1)\n","Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n","\u001b[K    100% |████████████████████████████████| 890kB 14.5MB/s \n","\u001b[?25hCollecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8)\n","  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n","Building wheels for collected packages: html5lib\n","  Building wheel for html5lib (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n","Successfully built html5lib\n","\u001b[31mmagenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.8.0 which is incompatible.\u001b[0m\n","Installing collected packages: html5lib, bleach, tensorboard, tensorflow\n","  Found existing installation: html5lib 1.0.1\n","    Uninstalling html5lib-1.0.1:\n","      Successfully uninstalled html5lib-1.0.1\n","  Found existing installation: bleach 3.1.0\n","    Uninstalling bleach-3.1.0:\n","      Successfully uninstalled bleach-3.1.0\n","  Found existing installation: tensorboard 1.13.1\n","    Uninstalling tensorboard-1.13.1:\n","      Successfully uninstalled tensorboard-1.13.1\n","  Found existing installation: tensorflow 1.13.1\n","    Uninstalling tensorflow-1.13.1:\n","      Successfully uninstalled tensorflow-1.13.1\n","Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.8.0 tensorflow-1.8.0\n"],"name":"stdout"}]},{"metadata":{"id":"79b4qqnwFf8D","colab_type":"code","outputId":"c626e68b-3d75-44ce-b0c7-198bdd246cc9","executionInfo":{"status":"ok","timestamp":1554814958310,"user_tz":-330,"elapsed":157825,"user":{"displayName":"Darshan Kakwani","photoUrl":"","userId":"06813604131485796128"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"TUsznNZOFFQz","colab_type":"code","colab":{}},"cell_type":"code","source":["# udc_metrics\n","\n","import tensorflow as tf\n","import functools\n","from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec\n","\n","\n","def create_evaluation_metrics():\n","    eval_metrics = {}\n","    for k in [1, 2, 5, 10]:\n","        eval_metrics[\"recall_at_%d\" % k] = MetricSpec(metric_fn=functools.partial(\n","            tf.contrib.metrics.streaming_sparse_recall_at_k,\n","            k=k))\n","    return eval_metrics"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VhJPHomjFM6r","colab_type":"code","colab":{}},"cell_type":"code","source":["# udc_inputs\n","\n","import tensorflow as tf\n","\n","TEXT_FEATURE_SIZE = 160\n","\n","def get_feature_columns(mode):\n","  feature_columns = []\n","\n","  feature_columns.append(tf.contrib.layers.real_valued_column(\n","    column_name=\"context\", dimension=TEXT_FEATURE_SIZE, dtype=tf.int64))\n","  feature_columns.append(tf.contrib.layers.real_valued_column(\n","      column_name=\"context_len\", dimension=1, dtype=tf.int64))\n","  feature_columns.append(tf.contrib.layers.real_valued_column(\n","      column_name=\"utterance\", dimension=TEXT_FEATURE_SIZE, dtype=tf.int64))\n","  feature_columns.append(tf.contrib.layers.real_valued_column(\n","      column_name=\"utterance_len\", dimension=1, dtype=tf.int64))\n","\n","  if mode == tf.contrib.learn.ModeKeys.TRAIN:\n","    # During training we have a label feature\n","    feature_columns.append(tf.contrib.layers.real_valued_column(\n","      column_name=\"label\", dimension=1, dtype=tf.int64))\n","\n","  if mode == tf.contrib.learn.ModeKeys.EVAL:\n","    # During evaluation we have distractors\n","    for i in range(9):\n","      feature_columns.append(tf.contrib.layers.real_valued_column(\n","        column_name=\"distractor_{}\".format(i), dimension=TEXT_FEATURE_SIZE, dtype=tf.int64))\n","      feature_columns.append(tf.contrib.layers.real_valued_column(\n","        column_name=\"distractor_{}_len\".format(i), dimension=1, dtype=tf.int64))\n","\n","  return set(feature_columns)\n","\n","\n","def create_input_fn(mode, input_files, batch_size, num_epochs):\n","  def input_fn():\n","    features = tf.contrib.layers.create_feature_spec_for_parsing(\n","        get_feature_columns(mode))\n","\n","    feature_map = tf.contrib.learn.io.read_batch_features(\n","        file_pattern=input_files,\n","        batch_size=batch_size,\n","        features=features,\n","        reader=tf.TFRecordReader,\n","        randomize_input=True,\n","        num_epochs=num_epochs,\n","        queue_capacity=200000 + batch_size * 10,\n","        name=\"read_batch_features_{}\".format(mode))\n","\n","    # This is an ugly hack because of a current bug in tf.learn\n","    # During evaluation TF tries to restore the epoch variable which isn't defined during training\n","    # So we define the variable manually here\n","    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n","      tf.get_variable(\n","        \"read_batch_features_eval/file_name_queue/limit_epochs/epochs\",\n","        initializer=tf.constant(0, dtype=tf.int64))\n","\n","    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n","      target = feature_map.pop(\"label\")\n","    else:\n","      # In evaluation we have 10 classes (utterances).\n","      # The first one (index 0) is always the correct one\n","      target = tf.zeros([batch_size, 1], dtype=tf.int64)\n","    return feature_map, target\n","  return input_fn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"71V8vTqhE9q9","colab_type":"code","colab":{}},"cell_type":"code","source":["# udc_hparams\n","\n","import tensorflow as tf\n","from collections import namedtuple\n","\n","# Model Parameters\n","tf.flags.DEFINE_integer(\n","  \"vocab_size\",\n","  91620,\n","  \"The size of the vocabulary. Only change this if you changed the preprocessing\")\n","\n","# Model Parameters\n","tf.flags.DEFINE_integer(\"embedding_dim\", 100, \"Dimensionality of the embeddings\")\n","tf.flags.DEFINE_integer(\"rnn_dim\", 256, \"Dimensionality of the RNN cell\")\n","tf.flags.DEFINE_integer(\"max_context_len\", 160, \"Truncate contexts to this length\")\n","tf.flags.DEFINE_integer(\"max_utterance_len\", 80, \"Truncate utterance to this length\")\n","\n","# Pre-trained embeddings\n","tf.flags.DEFINE_string(\"glove_path\", None, \"Path to pre-trained Glove vectors\")\n","tf.flags.DEFINE_string(\"vocab_path\", None, \"Path to vocabulary.txt file\")\n","\n","# Training Parameters\n","tf.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate\")\n","tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch size during training\")\n","tf.flags.DEFINE_integer(\"eval_batch_size\", 8, \"Batch size during evaluation\")\n","tf.flags.DEFINE_string(\"optimizer\", \"Adam\", \"Optimizer Name (Adam, Adagrad, etc)\")\n","\n","FLAGS = tf.flags.FLAGS\n","\n","HParams = namedtuple(\n","  \"HParams\",\n","  [\n","    \"batch_size\",\n","    \"embedding_dim\",\n","    \"eval_batch_size\",\n","    \"learning_rate\",\n","    \"max_context_len\",\n","    \"max_utterance_len\",\n","    \"optimizer\",\n","    \"rnn_dim\",\n","    \"vocab_size\",\n","    \"glove_path\",\n","    \"vocab_path\"\n","  ])\n","\n","def create_hparams():\n","  return HParams(\n","    batch_size=FLAGS.batch_size,\n","    eval_batch_size=FLAGS.eval_batch_size,\n","    vocab_size=FLAGS.vocab_size,\n","    optimizer=FLAGS.optimizer,\n","    learning_rate=FLAGS.learning_rate,\n","    embedding_dim=FLAGS.embedding_dim,\n","    max_context_len=FLAGS.max_context_len,\n","    max_utterance_len=FLAGS.max_utterance_len,\n","    glove_path=FLAGS.glove_path,\n","    vocab_path=FLAGS.vocab_path,\n","    rnn_dim=FLAGS.rnn_dim)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RpXMUyS7EtE7","colab_type":"code","colab":{}},"cell_type":"code","source":["# udc_model\n","\n","import tensorflow as tf\n","import sys\n","\n","def get_id_feature(features, key, len_key, max_len):\n","  ids = features[key]\n","  ids_len = tf.squeeze(features[len_key], [1])\n","  ids_len = tf.minimum(ids_len, tf.constant(max_len, dtype=tf.int64))\n","  return ids, ids_len\n","\n","def create_train_op(loss, hparams):\n","  train_op = tf.contrib.layers.optimize_loss(\n","      loss=loss,\n","      global_step=tf.contrib.framework.get_global_step(),\n","      learning_rate=hparams.learning_rate,\n","      clip_gradients=10.0,\n","      optimizer=hparams.optimizer)\n","  return train_op\n","\n","\n","def create_model_fn(hparams, model_impl):\n","\n","  def model_fn(features, targets, mode):\n","    context, context_len = get_id_feature(\n","        features, \"context\", \"context_len\", hparams.max_context_len)\n","\n","    utterance, utterance_len = get_id_feature(\n","        features, \"utterance\", \"utterance_len\", hparams.max_utterance_len)\n","\n","    if mode == tf.contrib.learn.ModeKeys.EVAL:\n","      _____batch_size = targets.get_shape().as_list()[0]\n","\n","    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n","      probs, loss = model_impl(\n","          hparams,\n","          mode,\n","          context,\n","          context_len,\n","          utterance,\n","          utterance_len,\n","          targets)\n","      train_op = create_train_op(loss, hparams)\n","      return probs, loss, train_op\n","\n","    if mode == tf.contrib.learn.ModeKeys.INFER:\n","      probs, loss = model_impl(\n","          hparams,\n","          mode,\n","          context,\n","          context_len,\n","          utterance,\n","          utterance_len,\n","          None)\n","      return probs, 0.0, None\n","\n","    if mode == tf.contrib.learn.ModeKeys.EVAL:\n","\n","      # We have 10 exampels per record, so we accumulate them\n","      all_contexts = [context]\n","      all_context_lens = [context_len]\n","      all_utterances = [utterance]\n","      all_utterance_lens = [utterance_len]\n","      all_targets = [tf.ones([_____batch_size, 1], dtype=tf.int64)]\n","\n","      for i in range(9):\n","        distractor, distractor_len = get_id_feature(features,\n","            \"distractor_{}\".format(i),\n","            \"distractor_{}_len\".format(i),\n","            hparams.max_utterance_len)\n","        all_contexts.append(context)\n","        all_context_lens.append(context_len)\n","        all_utterances.append(distractor)\n","        all_utterance_lens.append(distractor_len)\n","        all_targets.append(\n","          tf.zeros([_____batch_size, 1], dtype=tf.int64)\n","        )\n","\n","      probs, loss = model_impl(\n","          hparams,\n","          mode,\n","          tf.concat(all_contexts, 0),\n","          tf.concat(all_context_lens, 0),\n","          tf.concat(all_utterances, 0),\n","          tf.concat(all_utterance_lens, 0),\n","          tf.concat(all_targets, 0))\n","\n","      split_probs = tf.split(probs, 10, 0)\n","      shaped_probs = tf.concat(split_probs, 1)\n","\n","      # Add summaries\n","      tf.summary.histogram(\"eval_correct_probs_hist\", split_probs[0])\n","      tf.summary.scalar(\"eval_correct_probs_average\", tf.reduce_mean(split_probs[0]))\n","      tf.summary.histogram(\"eval_incorrect_probs_hist\", split_probs[1])\n","      tf.summary.scalar(\"eval_incorrect_probs_average\", tf.reduce_mean(split_probs[1]))\n","\n","      return shaped_probs, loss, None\n","\n","  return model_fn"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5QnobxYuGam9","colab_type":"code","colab":{}},"cell_type":"code","source":["# helpers\n","\n","import array\n","import numpy as np\n","import tensorflow as tf\n","from collections import defaultdict\n","\n","def load_vocab(filename):\n","  vocab = None\n","  with open(filename) as f:\n","    vocab = f.read().splitlines()\n","  dct = defaultdict(int)\n","  for idx, word in enumerate(vocab):\n","    dct[word] = idx\n","  return [vocab, dct]\n","\n","def load_glove_vectors(filename, vocab):\n","  \"\"\"\n","  Load glove vectors from a .txt file.\n","  Optionally limit the vocabulary to save memory. `vocab` should be a set.\n","  \"\"\"\n","  dct = {}\n","  vectors = array.array('d')\n","  current_idx = 0\n","  with open(filename, \"r\", encoding=\"utf-8\") as f:\n","    for _, line in enumerate(f):\n","      tokens = line.split(\" \")\n","      word = tokens[0]\n","      entries = tokens[1:]\n","      if not vocab or word in vocab:\n","        dct[word] = current_idx\n","        vectors.extend(float(x) for x in entries)\n","        current_idx += 1\n","    word_dim = len(entries)\n","    num_vectors = len(dct)\n","    tf.logging.info(\"Found {} out of {} vectors in Glove\".format(num_vectors, len(vocab)))\n","    return [np.array(vectors).reshape(num_vectors, word_dim), dct]\n","\n","\n","def build_initial_embedding_matrix(vocab_dict, glove_dict, glove_vectors, embedding_dim):\n","  initial_embeddings = np.random.uniform(-0.25, 0.25, (len(vocab_dict), embedding_dim)).astype(\"float32\")\n","  for word, glove_word_idx in glove_dict.items():\n","    word_idx = vocab_dict.get(word)\n","    initial_embeddings[word_idx, :] = glove_vectors[glove_word_idx]\n","  return initial_embeddings"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yjSmykwAGSEk","colab_type":"code","colab":{}},"cell_type":"code","source":["# dual_encoder\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","FLAGS = tf.flags.FLAGS\n","\n","def get_embeddings(hparams):\n","  if hparams.glove_path and hparams.vocab_path:\n","    tf.logging.info(\"Loading Glove embeddings...\")\n","    vocab_array, vocab_dict = helpers.load_vocab(hparams.vocab_path)\n","    glove_vectors, glove_dict = helpers.load_glove_vectors(hparams.glove_path, vocab=set(vocab_array))\n","    initializer = helpers.build_initial_embedding_matrix(vocab_dict, glove_dict, glove_vectors, hparams.embedding_dim)\n","  else:\n","    tf.logging.info(\"No glove/vocab path specificed, starting with random embeddings.\")\n","    initializer = tf.random_uniform_initializer(-0.25, 0.25)\n","\n","  return tf.get_variable(\n","    \"word_embeddings\",\n","    shape=[hparams.vocab_size, hparams.embedding_dim],\n","    initializer=initializer)\n","\n","\n","def dual_encoder_model(\n","    hparams,\n","    mode,\n","    context,\n","    context_len,\n","    utterance,\n","    utterance_len,\n","    targets):\n","\n","  # Initialize embedidngs randomly or with pre-trained vectors if available\n","  embeddings_W = get_embeddings(hparams)\n","\n","  # Embed the context and the utterance\n","  context_embedded = tf.nn.embedding_lookup(\n","      embeddings_W, context, name=\"embed_context\")\n","  utterance_embedded = tf.nn.embedding_lookup(\n","      embeddings_W, utterance, name=\"embed_utterance\")\n","\n","\n","  # Build the RNN\n","  with tf.variable_scope(\"rnn\") as vs:\n","    # We use an LSTM Cell\n","    cell = tf.contrib.rnn.BasicLSTMCell(\n","        hparams.rnn_dim,\n","        forget_bias=2.0,\n","        state_is_tuple=True)\n","\n","    # Run the utterance and context through the RNN\n","    rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\n","        cell,\n","        tf.concat([context_embedded, utterance_embedded], 0),\n","        sequence_length=tf.concat([context_len, utterance_len], 0),\n","        dtype=tf.float32)\n","    encoding_context, encoding_utterance = tf.split(rnn_states.h, 2, 0)\n","\n","  with tf.variable_scope(\"prediction\") as vs:\n","    M = tf.get_variable(\"M\",\n","      shape=[hparams.rnn_dim, hparams.rnn_dim],\n","      initializer=tf.truncated_normal_initializer())\n","\n","    # \"Predict\" a  response: c * M\n","    generated_response = tf.matmul(encoding_context, M)\n","    generated_response = tf.expand_dims(generated_response, 2)\n","    encoding_utterance = tf.expand_dims(encoding_utterance, 2)\n","\n","    # Dot product between generated response and actual response\n","    # (c * M) * r\n","    logits = tf.matmul(generated_response, encoding_utterance, True)\n","    logits = tf.squeeze(logits, [2])\n","\n","    # Apply sigmoid to convert logits to probabilities\n","    probs = tf.sigmoid(logits)\n","\n","    if mode == tf.contrib.learn.ModeKeys.INFER:\n","      return probs, None\n","\n","    # Calculate the binary cross-entropy loss\n","    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = tf.to_float(targets))\n","\n","  # Mean loss across the batch of examples\n","  mean_loss = tf.reduce_mean(losses, name=\"mean_loss\")\n","  return probs, mean_loss"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ta02OtZ4ExLL","colab_type":"code","outputId":"0fb270ba-58a0-4ec9-d360-9c848b353469","executionInfo":{"status":"error","timestamp":1554817173610,"user_tz":-330,"elapsed":2372984,"user":{"displayName":"Darshan Kakwani","photoUrl":"","userId":"06813604131485796128"}},"colab":{"base_uri":"https://localhost:8080/","height":4263}},"cell_type":"code","source":["import os\n","import time\n","import itertools\n","import sys\n","import tensorflow as tf\n","\n","tf.app.flags.DEFINE_string('f', '', 'kernel')\n","tf.app.flags.DEFINE_string(\"test_file\", \"drive/My Drive/BE Project/New Chatbot/data/test.tfrecords\", \"Path of test data in TFRecords format\")\n","tf.app.flags.DEFINE_string(\"model_dir\", \"drive/My Drive/BE Project/New Chatbot/runs/1554744310\", \"Directory to load model checkpoints from\")\n","tf.app.flags.DEFINE_integer(\"loglevel\", 20, \"Tensorflow log level\")\n","tf.app.flags.DEFINE_integer(\"test_batch_size\", 16, \"Batch size for testing\")\n","FLAGS = tf.app.flags.FLAGS\n","\n","if not FLAGS.model_dir:\n","  print(\"You must specify a model directory\")\n","  sys.exit(1)\n","\n","tf.logging.set_verbosity(FLAGS.loglevel)\n","\n","if __name__ == \"__main__\":\n","  hparams = create_hparams()\n","  model_fn = create_model_fn(hparams, model_impl=dual_encoder_model)\n","  estimator = tf.contrib.learn.Estimator(\n","    model_fn=model_fn,\n","    model_dir=FLAGS.model_dir,\n","    config=tf.contrib.learn.RunConfig())\n","\n","  input_fn_test = create_input_fn(\n","    mode=tf.contrib.learn.ModeKeys.EVAL,\n","    input_files=[FLAGS.test_file],\n","    batch_size=FLAGS.test_batch_size,\n","    num_epochs=1)\n","\n","  eval_metrics = create_evaluation_metrics()\n","  estimator.evaluate(input_fn=input_fn_test, steps=None, metrics=eval_metrics)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-9-b96080fac364>:26: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1179: BaseEstimator.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please replace uses of any Estimator from tf.contrib.learn with an Estimator from tf.estimator.*\n","INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff4ec6e4588>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_tf_config': gpu_options {\n","  per_process_gpu_memory_fraction: 1.0\n","}\n",", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'drive/My Drive/BE Project/New Chatbot/runs/1554744310'}\n","WARNING:tensorflow:From <ipython-input-3-e6fde66cd755>:12: MetricSpec.__init__ (from tensorflow.contrib.learn.python.learn.metric_spec) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.estimator.EstimatorSpec.eval_metric_ops.\n","WARNING:tensorflow:From <ipython-input-4-90c576b18bc4>:47: read_batch_features (from tensorflow.contrib.learn.python.learn.learn_io.graph_io) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.data.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py:833: read_keyed_batch_features (from tensorflow.contrib.learn.python.learn.learn_io.graph_io) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.data.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py:542: read_keyed_batch_examples (from tensorflow.contrib.learn.python.learn.learn_io.graph_io) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.data.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py:550: queue_parsed_features (from tensorflow.contrib.learn.python.learn.learn_io.graph_io) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.data.\n","INFO:tensorflow:No glove/vocab path specificed, starting with random embeddings.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py:1240: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one.\n","INFO:tensorflow:Starting evaluation at 2019-04-09-13:02:48\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from drive/My Drive/BE Project/New Chatbot/runs/1554744310/model.ckpt-1999\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [80,1] vs. [160,1]\n\t [[Node: prediction/logistic_loss/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](prediction/Squeeze, prediction/ToFloat)]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-b96080fac364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_evaluation_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 instructions)\n\u001b[0;32m--> 432\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    434\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, input_fn, feed_fn, batch_size, steps, metrics, name, checkpoint_path, hooks, log_progress)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         log_progress=log_progress)\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval_results\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_evaluate_model\u001b[0;34m(self, input_fn, steps, feed_fn, metrics, name, checkpoint_path, hooks, log_progress)\u001b[0m\n\u001b[1;32m    937\u001b[0m           \u001b[0mfinal_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m           config=self._session_config)\n\u001b[0m\u001b[1;32m    940\u001b[0m       \u001b[0mcurrent_global_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglobal_step_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py\u001b[0m in \u001b[0;36m_evaluate_once\u001b[0;34m(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval_ops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   logging.info('Finished evaluation at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    565\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1044\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [80,1] vs. [160,1]\n\t [[Node: prediction/logistic_loss/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](prediction/Squeeze, prediction/ToFloat)]]\n\nCaused by op 'prediction/logistic_loss/mul', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 832, in start\n    self._run_callback(self._callbacks.popleft())\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-b96080fac364>\", line 35, in <module>\n    estimator.evaluate(input_fn=input_fn_test, steps=None, metrics=eval_metrics)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 617, in evaluate\n    log_progress=log_progress)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 910, in _evaluate_model\n    model_fn_results = self._get_eval_ops(features, labels, metrics)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1292, in _get_eval_ops\n    model_fn_lib.ModeKeys.EVAL, metrics)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1227, in _call_model_fn\n    model_fn_results = self._model_fn(features, labels, **kwargs)\n  File \"<ipython-input-6-9e58a2fc4ba8>\", line 85, in model_fn\n    tf.concat(all_targets, 0))\n  File \"<ipython-input-8-e018045e18dc>\", line 80, in dual_encoder_model\n    losses = tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = tf.to_float(targets))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py\", line 181, in sigmoid_cross_entropy_with_logits\n    relu_logits - logits * labels,\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 979, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1211, in _mul_dispatch\n    return gen_math_ops.mul(x, y, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 4759, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [80,1] vs. [160,1]\n\t [[Node: prediction/logistic_loss/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](prediction/Squeeze, prediction/ToFloat)]]\n"]}]}]}