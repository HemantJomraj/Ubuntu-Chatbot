{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"jing.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"-u5MlwgZbFjH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"337cba18-260d-4925-a2e4-3040357f6531","executionInfo":{"status":"ok","timestamp":1554906490539,"user_tz":-330,"elapsed":6469,"user":{"displayName":"Darshan Kakwani","photoUrl":"","userId":"06813604131485796128"}}},"cell_type":"code","source":["!pip install tensorflow==0.12.1"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==0.12.1 in /usr/local/lib/python3.6/dist-packages (0.12.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==0.12.1) (1.14.6)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==0.12.1) (0.33.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==0.12.1) (1.11.0)\n","Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==0.12.1) (3.7.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==0.12.1) (40.9.0)\n"],"name":"stdout"}]},{"metadata":{"id":"_BElP6uhhHdW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9998308a-538e-482a-e989-0ec5886d40b7","executionInfo":{"status":"ok","timestamp":1554906495242,"user_tz":-330,"elapsed":11137,"user":{"displayName":"Darshan Kakwani","photoUrl":"","userId":"06813604131485796128"}}},"cell_type":"code","source":["!pip install bottle"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bottle in /usr/local/lib/python3.6/dist-packages (0.12.16)\n"],"name":"stdout"}]},{"metadata":{"id":"OnZlMDUVbU9D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c63aa6ec-9948-41d5-f0c8-cb29eaad0761","executionInfo":{"status":"ok","timestamp":1554906495246,"user_tz":-330,"elapsed":11087,"user":{"displayName":"Darshan Kakwani","photoUrl":"","userId":"06813604131485796128"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"lsRRp8XIbnma","colab_type":"code","colab":{}},"cell_type":"code","source":["#data_utils\n","\n","_PAD = \"_PAD\"\n","_GO = \"_GO\"\n","_EOS = \"_EOS\"\n","_UNK = \"_UNK\"\n","_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n","PAD_ID = 0\n","GO_ID = 1\n","EOS_ID = 2\n","UNK_ID = 3\n","OP_DICT_IDS = [PAD_ID, GO_ID, EOS_ID, UNK_ID]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jsT2UO16buR9","colab_type":"code","colab":{}},"cell_type":"code","source":["# corpora tools\n","\n","import pickle\n","import re\n","from collections import Counter\n","from nltk.corpus import comtrans\n","\n","def clean_sentence(sentence):\n","    regex_splitter = re.compile(\"([!?.,:;$\\\"\\')( ])\")\n","    clean_words = [re.split(regex_splitter, word.lower()) for word in sentence]\n","    return [w for words in clean_words for w in words if words if w]\n","\n","def filter_sentence_length(sentences_l1, sentences_l2, min_len=0, max_len=20):\n","    filtered_sentences_l1 = []\n","    filtered_sentences_l2 = []\n","    for i in range(len(sentences_l1)):\n","        if (len(sentences_l1[i]) <= max_len) and (len(sentences_l2[i]) <= max_len):\n","            filtered_sentences_l1.append(sentences_l1[i])\n","            filtered_sentences_l2.append(sentences_l2[i])\n","    return filtered_sentences_l1, filtered_sentences_l2\n","\n","#not sure about indent\n","def create_indexed_dictionary(sentences, dict_size=10000, storage_path=None):\n","    count_words = Counter()\n","    dict_words = {}\n","    opt_dict_size = len(OP_DICT_IDS)\n","    for sen in sentences:\n","        for word in sen:\n","            count_words[word] += 1\n","\n","    dict_words[_PAD] = PAD_ID\n","    dict_words[_GO] = GO_ID\n","    dict_words[_EOS] = EOS_ID\n","    dict_words[_UNK] = UNK_ID\n","\n","    for idx, item in enumerate(count_words.most_common(dict_size)):\n","        dict_words[item[0]] = idx + opt_dict_size\n","    if storage_path:\n","        pickle.dump(dict_words, open(storage_path, \"wb\"))\n","    return dict_words\n","\n","def sentences_to_indexes(sentences, indexed_dictionary):\n","    indexed_sentences = []\n","    not_found_counter = 0\n","    for sent in sentences:\n","        idx_sent = []\n","        for word in sent:\n","            try:\n","                idx_sent.append(indexed_dictionary[word])\n","            except KeyError:\n","                idx_sent.append(UNK_ID)\n","                not_found_counter += 1\n","        indexed_sentences.append(idx_sent)\n","    print('[sentences_to_indexes] Did not find {} words'.format(not_found_counter))\n","    return indexed_sentences\n","\n","def extract_max_length(corpora):\n","    return max([len(sentence) for sentence in corpora])\n","\n","def prepare_sentences(sentences_l1, sentences_l2, len_l1, len_l2):\n","    assert len(sentences_l1) == len(sentences_l2)\n","    data_set = []\n","    for i in range(len(sentences_l1)):\n","        padding_l1 = len_l1 - len(sentences_l1[i])\n","        pad_sentence_l1 = ([PAD_ID]*padding_l1) + sentences_l1[i]\n","        padding_l2 = len_l2 - len(sentences_l2[i])\n","        pad_sentence_l2 = [GO_ID] + sentences_l2[i] + [EOS_ID] + ([PAD_ID] * padding_l2)\n","        data_set.append([pad_sentence_l1, pad_sentence_l2])\n","    return data_set"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lqAs6gWLcevx","colab_type":"code","colab":{}},"cell_type":"code","source":["# corpora_get\n","\n","import pandas\n","\n","def read_conversations(file):\n","    conversations = [] #list of lists\n","    current_id = file['dialogueID'][0]\n","    current_conversation = []\n","    prev_user = \"\"\n","    for index, row in file.iterrows():\n","        if row['dialogueID'] != current_id:\n","            if len(current_conversation) > 1: #forming a conversation\n","                conversations.append(current_conversation)\n","            current_conversation = []\n","            prev_user = \"\"\n","            current_id = row['dialogueID']\n","        if row['from'] != prev_user:\n","            current_conversation.append(str(row['text']))\n","        else:\n","            current_conversation[len(current_conversation)-1] += str(row['text']) #one person talking continuously\n","        prev_user = row['from']\n","    return conversations\n","\n","def get_tokenized_sequencial_sentences(conversations):\n","   for conversation in conversations:\n","       for i in range (len(conversation)-1):\n","           yield (conversation[i].split(\" \"), conversation[i+1].split(\" \"))\n","       '''\n","       max = len(conversation) - 1\n","       i = 0\n","       while i < max:\n","           yield (conversation[i].split(\" \"), conversation[i+1].split(\" \"))\n","           i += 2\n","        '''\n","\n","    \n","def generate_conv_tuple(file):\n","    conversations = read_conversations(file)\n","    return tuple(zip(*list(get_tokenized_sequencial_sentences(conversations))))\n","\n","def get_ubuntu_corpus_data():\n","    file = pandas.read_csv('drive/My Drive/BE Project/dialogueText.csv')\n","    #file = (pandas.read_csv('Ubuntu-dialogue-corpus/dialogueText_196.csv'))\n","    #file.append(pandas.read_csv('Ubuntu-dialogue-corpus/dialogueText_301.csv'))\n","    return generate_conv_tuple(file)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YsWHxvhkcHpf","colab_type":"code","colab":{}},"cell_type":"code","source":["# seq2seq model\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import random\n","\n","import numpy as np\n","from six.moves import xrange  # pylint: disable=redefined-builtin\n","import tensorflow as tf\n","\n","from tensorflow.models.rnn.translate import data_utils\n","\n","\n","class Seq2SeqModel(object):\n","  \"\"\"Sequence-to-sequence model with attention and for multiple buckets.\n","  This class implements a multi-layer recurrent neural network as encoder,\n","  and an attention-based decoder. This is the same as the model described in\n","  this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n","  or into the seq2seq library for complete model implementation.\n","  This class also allows to use GRU cells in addition to LSTM cells, and\n","  sampled softmax to handle large output vocabulary size. A single-layer\n","  version of this model, but with bi-directional encoder, was presented in\n","    http://arxiv.org/abs/1409.0473\n","  and sampled softmax is described in Section 3 of the following paper.\n","    http://arxiv.org/abs/1412.2007\n","  \"\"\"\n","\n","  def __init__(self, source_vocab_size, target_vocab_size, buckets, size,\n","               num_layers, max_gradient_norm, batch_size, learning_rate,\n","               learning_rate_decay_factor, use_lstm=False,\n","               num_samples=512, forward_only=False):\n","    \"\"\"Create the model.\n","    Args:\n","      source_vocab_size: size of the source vocabulary.\n","      target_vocab_size: size of the target vocabulary.\n","      buckets: a list of pairs (I, O), where I specifies maximum input length\n","        that will be processed in that bucket, and O specifies maximum output\n","        length. Training instances that have inputs longer than I or outputs\n","        longer than O will be pushed to the next bucket and padded accordingly.\n","        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n","      size: number of units in each layer of the model.\n","      num_layers: number of layers in the model.\n","      max_gradient_norm: gradients will be clipped to maximally this norm.\n","      batch_size: the size of the batches used during training;\n","        the model construction is independent of batch_size, so it can be\n","        changed after initialization if this is convenient, e.g., for decoding.\n","      learning_rate: learning rate to start with.\n","      learning_rate_decay_factor: decay learning rate by this much when needed.\n","      use_lstm: if true, we use LSTM cells instead of GRU cells.\n","      num_samples: number of samples for sampled softmax.\n","      forward_only: if set, we do not construct the backward pass in the model.\n","    \"\"\"\n","    self.source_vocab_size = source_vocab_size\n","    self.target_vocab_size = target_vocab_size\n","    self.buckets = buckets\n","    self.batch_size = batch_size\n","    self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n","    self.learning_rate_decay_op = self.learning_rate.assign(\n","        self.learning_rate * learning_rate_decay_factor)\n","    self.global_step = tf.Variable(0, trainable=False)\n","\n","    # If we use sampled softmax, we need an output projection.\n","    output_projection = None\n","    softmax_loss_function = None\n","    # Sampled softmax only makes sense if we sample less than vocabulary size.\n","    if num_samples > 0 and num_samples < self.target_vocab_size:\n","      with tf.device(\"/cpu:0\"):\n","        w = tf.get_variable(\"proj_w\", [size, self.target_vocab_size])\n","        w_t = tf.transpose(w)\n","        b = tf.get_variable(\"proj_b\", [self.target_vocab_size])\n","      output_projection = (w, b)\n","\n","      def sampled_loss(inputs, labels):\n","        with tf.device(\"/cpu:0\"):\n","          labels = tf.reshape(labels, [-1, 1])\n","          return tf.nn.sampled_softmax_loss(w_t, b, inputs, labels, num_samples,\n","                                            self.target_vocab_size)\n","      softmax_loss_function = sampled_loss\n","\n","    # Create the internal multi-layer cell for our RNN.\n","    single_cell = tf.nn.rnn_cell.GRUCell(size)\n","    if use_lstm:\n","      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\n","    cell = single_cell\n","    if num_layers > 1:\n","      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n","\n","    # The seq2seq function: we use embedding for the input and attention.\n","    def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n","      return tf.nn.seq2seq.embedding_attention_seq2seq(\n","          encoder_inputs, decoder_inputs, cell,\n","          num_encoder_symbols=source_vocab_size,\n","          num_decoder_symbols=target_vocab_size,\n","          embedding_size=size,\n","          output_projection=output_projection,\n","          feed_previous=do_decode)\n","\n","    # Feeds for inputs.\n","    self.encoder_inputs = []\n","    self.decoder_inputs = []\n","    self.target_weights = []\n","    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n","      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n","                                                name=\"encoder{0}\".format(i)))\n","    for i in xrange(buckets[-1][1] + 1):\n","      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n","                                                name=\"decoder{0}\".format(i)))\n","      self.target_weights.append(tf.placeholder(tf.float32, shape=[None],\n","                                                name=\"weight{0}\".format(i)))\n","\n","    # Our targets are decoder inputs shifted by one.\n","    targets = [self.decoder_inputs[i + 1]\n","               for i in xrange(len(self.decoder_inputs) - 1)]\n","\n","    # Training outputs and losses.\n","    if forward_only:\n","      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n","          self.encoder_inputs, self.decoder_inputs, targets,\n","          self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n","          softmax_loss_function=softmax_loss_function)\n","      # If we use output projection, we need to project outputs for decoding.\n","      if output_projection is not None:\n","        for b in xrange(len(buckets)):\n","          self.outputs[b] = [\n","              tf.matmul(output, output_projection[0]) + output_projection[1]\n","              for output in self.outputs[b]\n","          ]\n","    else:\n","      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n","          self.encoder_inputs, self.decoder_inputs, targets,\n","          self.target_weights, buckets,\n","          lambda x, y: seq2seq_f(x, y, False),\n","          softmax_loss_function=softmax_loss_function)\n","\n","    # Gradients and SGD update operation for training the model.\n","    params = tf.trainable_variables()\n","    if not forward_only:\n","      self.gradient_norms = []\n","      self.updates = []\n","      opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n","      for b in xrange(len(buckets)):\n","        gradients = tf.gradients(self.losses[b], params)\n","        clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n","                                                         max_gradient_norm)\n","        self.gradient_norms.append(norm)\n","        self.updates.append(opt.apply_gradients(\n","            zip(clipped_gradients, params), global_step=self.global_step))\n","\n","    self.saver = tf.train.Saver(tf.all_variables())\n","\n","  def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n","           bucket_id, forward_only):\n","    \"\"\"Run a step of the model feeding the given inputs.\n","    Args:\n","      session: tensorflow session to use.\n","      encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n","      decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n","      target_weights: list of numpy float vectors to feed as target weights.\n","      bucket_id: which bucket of the model to use.\n","      forward_only: whether to do the backward step or only forward.\n","    Returns:\n","      A triple consisting of gradient norm (or None if we did not do backward),\n","      average perplexity, and the outputs.\n","    Raises:\n","      ValueError: if length of encoder_inputs, decoder_inputs, or\n","        target_weights disagrees with bucket size for the specified bucket_id.\n","    \"\"\"\n","    # Check if the sizes match.\n","    encoder_size, decoder_size = self.buckets[bucket_id]\n","    if len(encoder_inputs) != encoder_size:\n","      raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n","                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n","    if len(decoder_inputs) != decoder_size:\n","      raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n","                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n","    if len(target_weights) != decoder_size:\n","      raise ValueError(\"Weights length must be equal to the one in bucket,\"\n","                       \" %d != %d.\" % (len(target_weights), decoder_size))\n","\n","    # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n","    input_feed = {}\n","    for l in xrange(encoder_size):\n","      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n","    for l in xrange(decoder_size):\n","      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n","      input_feed[self.target_weights[l].name] = target_weights[l]\n","\n","    # Since our targets are decoder inputs shifted by one, we need one more.\n","    last_target = self.decoder_inputs[decoder_size].name\n","    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n","\n","    # Output feed: depends on whether we do a backward step or not.\n","    if not forward_only:\n","      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n","                     self.gradient_norms[bucket_id],  # Gradient norm.\n","                     self.losses[bucket_id]]  # Loss for this batch.\n","    else:\n","      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n","      for l in xrange(decoder_size):  # Output logits.\n","        output_feed.append(self.outputs[bucket_id][l])\n","\n","    outputs = session.run(output_feed, input_feed)\n","    if not forward_only:\n","      return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n","    else:\n","      return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n","\n","  def get_batch(self, data, bucket_id):\n","    \"\"\"Get a random batch of data from the specified bucket, prepare for step.\n","    To feed data in step(..) it must be a list of batch-major vectors, while\n","    data here contains single length-major cases. So the main logic of this\n","    function is to re-index data cases to be in the proper format for feeding.\n","    Args:\n","      data: a tuple of size len(self.buckets) in which each element contains\n","        lists of pairs of input and output data that we use to create a batch.\n","      bucket_id: integer, which bucket to get the batch for.\n","    Returns:\n","      The triple (encoder_inputs, decoder_inputs, target_weights) for\n","      the constructed batch that has the proper format to call step(...) later.\n","    \"\"\"\n","    encoder_size, decoder_size = self.buckets[bucket_id]\n","    encoder_inputs, decoder_inputs = [], []\n","\n","    # Get a random batch of encoder and decoder inputs from data,\n","    # pad them if needed, reverse encoder inputs and add GO to decoder.\n","    for _ in xrange(self.batch_size):\n","      encoder_input, decoder_input = random.choice(data[bucket_id])\n","\n","      # Encoder inputs are padded and then reversed.\n","      encoder_pad = [PAD_ID] * (encoder_size - len(encoder_input))\n","      encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n","\n","      # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n","      decoder_pad_size = decoder_size - len(decoder_input) - 1\n","      decoder_inputs.append([GO_ID] + decoder_input +\n","                            [PAD_ID] * decoder_pad_size)\n","\n","    # Now we create batch-major vectors from the data selected above.\n","    batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n","\n","    # Batch encoder inputs are just re-indexed encoder_inputs.\n","    for length_idx in xrange(encoder_size):\n","      batch_encoder_inputs.append(\n","          np.array([encoder_inputs[batch_idx][length_idx]\n","                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n","\n","    # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n","    for length_idx in xrange(decoder_size):\n","      batch_decoder_inputs.append(\n","          np.array([decoder_inputs[batch_idx][length_idx]\n","                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n","\n","      # Create target_weights to be 0 for targets that are padding.\n","      batch_weight = np.ones(self.batch_size, dtype=np.float32)\n","      for batch_idx in xrange(self.batch_size):\n","        # We set weight to 0 if the corresponding target is a PAD symbol.\n","        # The corresponding target is decoder_input shifted by 1 forward.\n","        if length_idx < decoder_size - 1:\n","          target = decoder_inputs[batch_idx][length_idx + 1]\n","        if length_idx == decoder_size - 1 or target == PAD_ID:\n","          batch_weight[batch_idx] = 0.0\n","      batch_weights.append(batch_weight)\n","    return batch_encoder_inputs, batch_decoder_inputs, batch_weights"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wjFO7C8BcAPu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"507a1c2f-108e-4e75-e4ef-896c49d555d1","executionInfo":{"status":"ok","timestamp":1554906633952,"user_tz":-330,"elapsed":149634,"user":{"displayName":"Darshan Kakwani","photoUrl":"","userId":"06813604131485796128"}}},"cell_type":"code","source":["# train_chatbot\n","\n","import time\n","import math\n","import sys\n","import pickle\n","import glob\n","import os\n","import tensorflow as tf\n","path_l1_dict = \"drive/My Drive/BE Project/jing/tmp/l1_dict.p\"\n","path_l2_dict = \"drive/My Drive/BE Project/jing/tmp/l2_dict.p\"\n","model_dir = \"drive/My Drive/BE Project/jing/tmp/chat\"\n","model_checkpoints = model_dir + \"/chat.ckpt\"\n","\n","def build_dataset(use_stored_dictionary=False):\n","    sen_l1, sen_l2 = get_ubuntu_corpus_data()\n","    clean_sen_l1 = [clean_sentence(s) for s in sen_l1] ### OTHERWISE IT DOES NOT RUN ON MY LAPTOP\n","    clean_sen_l2 = [clean_sentence(s) for s in sen_l2] ### OTHERWISE IT DOES NOT RUN ON MY LAPTOP\n","    filt_clean_sen_l1, filt_clean_sen_l2 = filter_sentence_length(clean_sen_l1, clean_sen_l2, max_len=20)\n","    if not use_stored_dictionary:\n","        #change dict_size according to input size\n","        dict_l1 = create_indexed_dictionary(filt_clean_sen_l1, dict_size=20000, storage_path=path_l1_dict)\n","        dict_l2 = create_indexed_dictionary(filt_clean_sen_l2, dict_size=20000, storage_path=path_l2_dict)\n","    else:\n","        dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n","        dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n","    dict_l1_length = len(dict_l1)\n","    dict_l2_length = len(dict_l2)\n","    idx_sentences_l1 = sentences_to_indexes(filt_clean_sen_l1, dict_l1)\n","    idx_sentences_l2 = sentences_to_indexes(filt_clean_sen_l2, dict_l2)\n","    max_length_l1 = extract_max_length(idx_sentences_l1)\n","    max_length_l2 = extract_max_length(idx_sentences_l2)\n","    data_set = prepare_sentences(idx_sentences_l1, idx_sentences_l2, max_length_l1, max_length_l2)\n","    return (filt_clean_sen_l1, filt_clean_sen_l2), data_set, (max_length_l1, max_length_l2), (dict_l1_length, dict_l2_length)\n","\n","def cleanup_checkpoints(model_dir, model_checkpoints):\n","    for f in glob.glob(model_checkpoints + \"*\"):\n","        os.remove(f)\n","    try:\n","        os.mkdir(model_dir)\n","    except FileExistsError:\n","        pass\n","\n","def get_seq2seq_model(session, forward_only, dict_lengths, max_sentence_lengths, model_dir):\n","    model = Seq2SeqModel(\n","            source_vocab_size=dict_lengths[0],\n","            target_vocab_size=dict_lengths[1],\n","            buckets=[max_sentence_lengths],\n","            size=256,\n","            num_layers=2,\n","            max_gradient_norm=5.0,\n","            batch_size=128,\n","            learning_rate=1.0,\n","            learning_rate_decay_factor=0.99,\n","            forward_only=forward_only)\n","            #dtype=tf.float16)\n","    ckpt = tf.train.get_checkpoint_state(model_dir)\n","    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n","        print(\"Reading model parameters from {}\".format(ckpt.model_checkpoint_path))\n","        model.saver.restore(session, ckpt.model_checkpoint_path)\n","    else:\n","        print(\"Created model with fresh parameters.\")\n","        session.run(tf.global_variables_initializer())\n","    return model\n","\n","'''\n","def train():\n","    with tf.Session() as sess:\n","        model = get_seq2seq_model(sess, False, dict_lengths, max_sentence_lengths, model_dir)\n","        # This is the training loop.\n","        step_time, loss = 0.0, 0.0\n","        current_step = 0\n","        bucket = 0\n","        steps_per_checkpoint = 100\n","        max_steps = 10000 #change to a larger number later\n","        while current_step < max_steps:\n","            start_time = time.time()\n","            encoder_inputs, decoder_inputs, target_weights = model.get_batch([data_set], bucket)\n","            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket, False)\n","            step_time += (time.time() - start_time) / steps_per_checkpoint\n","            loss += step_loss / steps_per_checkpoint\n","            current_step += 1\n","            if current_step % steps_per_checkpoint == 0:\n","                perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n","                print (\"global step {} learning rate {} step_time {} perplexity {}\".format(\n","                    model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity))\n","                sess.run (model.learning_rate_decay_op)\n","                model.saver.save(sess, model_checkpoints, global_step = model.global_step)\n","                step_time, loss = 0.0, 0.0\n","                encoder_inputs, decoder_inputs, target_weights = model.get_batch([data_set], bucket)\n","                _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket, True)\n","                eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n","                print (\" eval: perplexity {}\".format(eval_ppx))\n","                sys.stdout.flush()\n","'''\n","if __name__ == \"__main__\":\n","    _, data_set, max_sentence_lengths, dict_lengths = build_dataset(False)\n","    cleanup_checkpoints(model_dir, model_checkpoints)\n","    #train()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[sentences_to_indexes] Did not find 88975 words\n","[sentences_to_indexes] Did not find 60599 words\n"],"name":"stdout"}]},{"metadata":{"id":"Ak8BRaHWbfF2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"663a016e-a7d9-43ee-e2ad-dcbf09f397c9","executionInfo":{"status":"ok","timestamp":1554906802857,"user_tz":-330,"elapsed":318512,"user":{"displayName":"Darshan Kakwani","photoUrl":"","userId":"06813604131485796128"}}},"cell_type":"code","source":["# test_chatbot\n","\n","import pickle\n","import sys\n","import os\n","import numpy as np\n","import tensorflow as tf\n","model_dir = \"drive/My Drive/BE Project/jing/tmp/chat\"\n","path_l1_dict = \"drive/My Drive/BE Project/jing/tmp/l1_dict.p\"\n","path_l2_dict = \"drive/My Drive/BE Project/jing/tmp/l2_dict.p\"\n","\n","def prepare_sentence(sentence, dict_l1, max_length):\n","   sents = [sentence.split(\" \")]\n","   clean_sen_l1 = [clean_sentence(s) for s in sents]\n","   idx_sentences_l1 = sentences_to_indexes(clean_sen_l1, dict_l1)\n","   data_set = prepare_sentences(idx_sentences_l1, [[]], max_length, max_length)\n","   sentences = (clean_sen_l1, [[]])\n","   return sentences, data_set\n","\n","def decode(data_set):\n","    with tf.Session() as sess:\n","        model = get_seq2seq_model(sess, True, dict_lengths, max_sentence_lengths, model_dir)\n","        model.batch_size = 1\n","        bucket = 0\n","        encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n","         {bucket: [(data_set[0][0], [])]}, bucket)\n","        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n","                                       target_weights, bucket, True)\n","        outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n","        if EOS_ID in outputs:\n","            outputs = outputs[1:outputs.index(EOS_ID)]\n","    tf.reset_default_graph()\n","    return \" \".join([tf.compat.as_str(inv_dict_l2[output]) for output in outputs])\n","\n","if __name__ == \"__main__\":\n","    dict_l1 = pickle.load(open(path_l1_dict, \"rb\"))\n","    dict_l1_length = len(dict_l1)\n","    dict_l2 = pickle.load(open(path_l2_dict, \"rb\"))\n","    dict_l2_length = len(dict_l2)\n","    inv_dict_l2 = {v: k for k, v in dict_l2.items()}\n","    max_lengths = 10\n","    dict_lengths = (dict_l1_length, dict_l2_length)\n","    max_sentence_lengths = (max_lengths, max_lengths)\n","\n","    #from bottle import route, run, request\n","    #@route('/api')\n","    #def api():\n","    in_sentence = input(\"You: \")\n","    #print (in_sentence)\n","    _, data_set = prepare_sentence(in_sentence, dict_l1, max_lengths)\n","    resp = [{\"in\": in_sentence, \"out\": decode(data_set)}]\n","    #return dict(data=resp)\n","    print(\"Chatbot: \" + resp[0].get(\"out\"))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["You: guys help how can i delete a file where permission is denied\n","[sentences_to_indexes] Did not find 0 words\n","WARNING:tensorflow:From <ipython-input-7-372ea92ef85a>:150 in __init__.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Please use tf.global_variables instead.\n","Created model with fresh parameters.\n","Chatbot: trashed trashed trashed qwerty qwerty qwerty qwerty qwerty qwerty hao\n"],"name":"stdout"}]}]}